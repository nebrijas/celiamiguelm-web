{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edcc461f",
   "metadata": {},
   "source": [
    "# Prgramación literaria con Jupyter (ad3)\n",
    "## Web Scraping con Python\n",
    "Este programa se encarga de hacer **web scraping** en varias URLs de una misma web. Los resultados son almacenados y clasificados por palabras clave para imprimirlos al final del programa por consola."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f05532",
   "metadata": {},
   "source": [
    "### Librerías utilizadas\n",
    "+ requests: [requests es una librería Python que facilita enormemente el trabajo con peticiones HTTP.](https://j2logo.com/python/python-requests-peticiones-http/)\n",
    "+ bs4: [Beautiful Soup es una librería Python que permite extraer información de contenido en formato HTML o XML.](https://j2logo.com/python/web-scraping-con-python-guia-inicio-beautifulsoup/)\n",
    "+ pandas: [Pandas es una librería de Python especializada en el manejo y análisis de estructuras de datos.](https://aprendeconalf.es/docencia/python/manual/pandas/)\n",
    "+ termcolor: [formato de color ANSII para salida en terminal.](https://pypi.org/project/termcolor/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717183b8",
   "metadata": {},
   "source": [
    "### Módulos utilizados\n",
    "+ time: [El módulo time de la biblioteca estándar de Python proporciona un conjunto de funciones para trabajar con fechas y/o horas.](https://python-para-impacientes.blogspot.com/2017/03/el-modulo-time.html)\n",
    "+ csv: [El módulo csv implementa clases para leer y escribir datos tabulares en formato CSV.](https://docs.python.org/es/3/library/csv.html)\n",
    "+ re: [Este módulo proporciona operaciones de coincidencia de expresiones regulares similares a las encontradas en Perl.](https://docs.python.org/es/3/library/re.html)\n",
    "+ os: [Este módulo provee una manera versátil de usar funcionalidades dependientes del sistema operativo.](https://docs.python.org/es/3.10/library/os.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ebee39",
   "metadata": {},
   "source": [
    "### Instalar las librerías necesarias\n",
    "+ pandas: `pip3 install pandas`\n",
    "+ requests: `python -m pip install requests`\n",
    "+ bs4: `pip install bs4`\n",
    "+ termcolor: `pip3 install termcolor`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6669f0b1",
   "metadata": {},
   "source": [
    "### Descripción del programa\n",
    "El primer paso, como en cualquier programa, es importar las librerías que va a necesitar el script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ee9a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54f663d",
   "metadata": {},
   "source": [
    "Se declara la variable `resultados` y se inicializa como un array vacío. En ella se van almacenando los resultados de cada una de las URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77bf33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a8ba57",
   "metadata": {},
   "source": [
    "#### Extraer los resultados de las URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b0ef6c",
   "metadata": {},
   "source": [
    "Después, se realiza una petición **HTTP GET** a la URL de la web donde se quiere hacer web scraping. Esta petición devuelve una respuesta que se almacena en una variable llamada `req`. Esta variable es de tipo objeto y contiene tanto atributos como funciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42a95a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "req = requests.get(\"https://resultados.elpais.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8161ba78",
   "metadata": {},
   "source": [
    "Para comprobar que la petición se ha realizado con éxito, se utiliza uno de los atributos disponibles en el objeto `req` llamado `status_code`. Si el valor es igual a **200**, la petición se ha realizado con éxito. En caso contrario, se lanza una excepción para avisar cual ha sido la URL que ha fallado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00459c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (req.status_code != 200):\n",
    "    raise Exception(\"No se puede hacer Web Scraping en\"+ URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fbfa7b",
   "metadata": {},
   "source": [
    "Una vez se comprueba que la petición ha sido exitosa, se extrae todo el texto HTML de la página web con ayuda de una de las funciones de la librería **bs4**: `BeautifulSoup`. En esta función se le pasan dos parámetros: el texto obtenido de la petición HTTP a la web, y el tipo de texto que tiene que extraer. El resultado se almacena en una variable llamada `soup`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950e1d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(req.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0c894b",
   "metadata": {},
   "source": [
    "Como el objetivo es obtener los titulares de cada URL, tan solo es necesario el texto de las etiqueteas **<h2>**. Para hacer el filtrado, se utiliza una de las funciones que contiene el objeto `soup` llamada `findAll`. De esta manera solo se almacenan en la variable `tags` el texto de los titulares de la web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c73ac7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = soup.findAll(\"h2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c904b65d",
   "metadata": {},
   "source": [
    "La variable `tags` almacena todos los titulares obtenidos en un array. Para almacenarlos individualmente en la variable `resultados`, se recorren con una sentencia **for** y se añaden al final del array en cada iteración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeac4225",
   "metadata": {},
   "outputs": [],
   "source": [
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc42023d",
   "metadata": {},
   "source": [
    "Todo el proceso realizado hasta ahora se repite el número de veces necesario para extraer los titulares de las distintas URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0818fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "req2 = requests.get(\"https://elpais.com/internacional\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup2 = BeautifulSoup(req2.text, 'html.parser')\n",
    "\n",
    "tags = soup2.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req3 = requests.get(\"https://elpais.com/opinion\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup3 = BeautifulSoup(req3.text, 'html.parser')\n",
    "\n",
    "tags = soup3.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req4 = requests.get(\"https://elpais.com/espana/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup4 = BeautifulSoup(req4.text, 'html.parser')\n",
    "\n",
    "tags = soup4.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req5 = requests.get(\"https://elpais.com/economia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup5 = BeautifulSoup(req5.text, 'html.parser')\n",
    "\n",
    "tags = soup5.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req6 = requests.get(\"https://elpais.com/sociedad/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup6 = BeautifulSoup(req6.text, 'html.parser')\n",
    "\n",
    "tags = soup6.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req7 = requests.get(\"https://elpais.com/educacion/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup7 = BeautifulSoup(req7.text, 'html.parser')\n",
    "\n",
    "tags = soup7.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req8 = requests.get(\"https://elpais.com/clima-y-medio-ambiente/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup8 = BeautifulSoup(req8.text, 'html.parser')\n",
    "\n",
    "tags = soup8.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req9 = requests.get(\"https://elpais.com/ciencia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup9 = BeautifulSoup(req9.text, 'html.parser')\n",
    "\n",
    "tags = soup9.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req10 = requests.get(\"https://elpais.com/cultura/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup10 = BeautifulSoup(req10.text, 'html.parser')\n",
    "\n",
    "tags = soup10.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req11 = requests.get(\"https://elpais.com/babelia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup11 = BeautifulSoup(req11.text, 'html.parser')\n",
    "\n",
    "tags = soup11.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req12 = requests.get(\"https://elpais.com/deportes/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup12 = BeautifulSoup(req12.text, 'html.parser')\n",
    "\n",
    "tags = soup12.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req13 = requests.get(\"https://elpais.com/tecnologia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup13 = BeautifulSoup(req13.text, 'html.parser')\n",
    "\n",
    "tags = soup13.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req14 = requests.get(\"https://elpais.com/tecnologia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup14 = BeautifulSoup(req14.text, 'html.parser')\n",
    "\n",
    "tags = soup14.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req15 = requests.get(\"https://elpais.com/gente/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup15 = BeautifulSoup(req15.text, 'html.parser')\n",
    "\n",
    "tags = soup15.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req16 = requests.get(\"https://elpais.com/television/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup16 = BeautifulSoup(req16.text, 'html.parser')\n",
    "\n",
    "tags = soup16.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req17 = requests.get(\"https://elpais.com/eps/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup17 = BeautifulSoup(req17.text, 'html.parser')\n",
    "\n",
    "tags = soup17.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fc331a",
   "metadata": {},
   "source": [
    "Antes de mostrar los resultados por la termial, se envía el comando `clear` al sistema para que la consola quede limpia y se puedan apreciar bien los titulares extraídos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4831702",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"clear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38030250",
   "metadata": {},
   "source": [
    "Se muestran por consola las categorías por las que se van a clasificar los titulares extraídos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531b0643",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(colored(\"A continuación se muestran los titulares de las páginas principales del diario El País que contienen las siguientes palabras:\", 'blue', attrs=['bold']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b7431e",
   "metadata": {},
   "source": [
    "#### Mostrar los titulares extraídos por categoría"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820546b5",
   "metadata": {},
   "source": [
    "Se muestra el título de la categoría en la consola."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a03a53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(colored(\"Feminismo\", 'green', attrs=['bold']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6668e82",
   "metadata": {},
   "source": [
    "Para filtrar los titulares por categoría se utiliza un bucle `for`. Este en cada interacción comprueba si el titular contiene la palabra clave, y si es así, guarda el titular en la variable `str_match`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6a9008",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_match = [s for s in resultados if \"feminismo\" in s]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a871d2c",
   "metadata": {},
   "source": [
    "Por último, se muestran todos los titulares de la categoría en concreto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312015f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\".join(str_match))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed49305",
   "metadata": {},
   "source": [
    "Al igual que antes, este último bloque de código se repite tantas veces como categorías se definan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1890885b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(colored(\"Igualdad\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"igualdad\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Mujeres\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"mujeres\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Mujer\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"mujer\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Brecha salarial\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"brecha salarial\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Machismo\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"machismo\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Violencia\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"violencia\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Maltrato\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"maltrato\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Homicidio\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"homicidio\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Género\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"género\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Asesinato\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"asesinato\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Sexo\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"sexo\" in s]\n",
    "print(\"\\n\".join(str_match))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
